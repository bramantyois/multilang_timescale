{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/antares_raid/home/bramantyos/codes/multilang_timescale\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# move to project root\n",
    "while True:\n",
    "    # get list of directories\n",
    "    dirs = os.listdir()\n",
    "    if \"README.md\" in dirs:\n",
    "        break\n",
    "    else:\n",
    "        os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from src.utils.plot import config_plotting, figsize_dict, create_discrete_cmap\n",
    "from src.utils.meta import read_result_meta\n",
    "from src.utils.utils import get_valid_voxels\n",
    "from src.utils.weight import process_primal_weight\n",
    "from src.settings import TrainerSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_plotting('paper')\n",
    "# get default font size\n",
    "default_font_size = plt.rcParams['font.size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \".temp/image/semantic_cluster/\"\n",
    "\n",
    "result_metric = \"r2\"    \n",
    "result_meta_dir = \".temp/result_meta/bling\"\n",
    "alpha = 0.05\n",
    "r2_threshold = 0.1\n",
    "\n",
    "cluster_centroids_file = \".temp/misc/semantic_cluster/cluster_centroids_bling.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ids = ['COL', 'GFW', 'TYE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_semantic_weight_data(subject_id):\n",
    "    subject_file_en = f\".temp/config/bling/subject/{subject_id}_en.json\"\n",
    "    subject_file_zh = f\".temp/config/bling/subject/{subject_id}_zh.json\"\n",
    "\n",
    "    feature_file_en = f\".temp/config/bling/feature/{subject_id}/fasttext_stepwise_en.json\"\n",
    "    feature_file_zh = f\".temp/config/bling/feature/{subject_id}/fasttext_stepwise_zh.json\"\n",
    "\n",
    "    trainer_en_file = (\n",
    "        f\".temp/config/bling/train/stepwise/{subject_id.lower()}_en-save_primal.json\"\n",
    "    )\n",
    "    trainer_zh_file = (\n",
    "        f\".temp/config/bling/train/stepwise/{subject_id.lower()}_zh-save_primal.json\"\n",
    "    )\n",
    "    # loading semantic vem\n",
    "\n",
    "    en_meta_df = read_result_meta(\n",
    "        result_meta_dir,\n",
    "        trainer_setting_path=trainer_en_file,\n",
    "        subject_setting_path=subject_file_en,\n",
    "        feature_setting_path=feature_file_en,\n",
    "    )\n",
    "    zh_meta_df = read_result_meta(\n",
    "        result_meta_dir,\n",
    "        trainer_setting_path=trainer_zh_file,\n",
    "        subject_setting_path=subject_file_zh,\n",
    "        feature_setting_path=feature_file_zh,\n",
    "    )\n",
    "        \n",
    "    en_config = en_meta_df.iloc[0].to_dict()\n",
    "    zh_config = zh_meta_df.iloc[0].to_dict()\n",
    "\n",
    "    en_stat = en_config[\"stats_path\"]\n",
    "    en_stat = np.load(en_stat)\n",
    "\n",
    "    zh_stat = zh_config[\"stats_path\"]\n",
    "    zh_stat = np.load(zh_stat)\n",
    "\n",
    "    valid_en_voxel_mask, valid_en_voxel_idx = get_valid_voxels(en_stat, metric=result_metric, score_threshold=r2_threshold, alpha=alpha)\n",
    "    valid_zh_voxel_mask, valid_zh_voxel_idx = get_valid_voxels(zh_stat, metric=result_metric, score_threshold=r2_threshold, alpha=alpha)\n",
    "\n",
    "    en_joint_pred_acc = en_stat[f\"test_joint_{result_metric}_score_mask\"]\n",
    "    zh_joint_pred_acc = zh_stat[f\"test_joint_{result_metric}_score_mask\"]\n",
    "\n",
    "    # # get sqrt\n",
    "    if result_metric == 'r2':\n",
    "        en_joint_pred_acc = np.sqrt(np.maximum(en_joint_pred_acc, 0))\n",
    "        zh_joint_pred_acc = np.sqrt(np.maximum(zh_joint_pred_acc, 0))\n",
    "\n",
    "    return {\n",
    "        'en': {\n",
    "            'config': en_config,\n",
    "            'stat': en_stat,\n",
    "            'valid_voxel_mask': valid_en_voxel_mask,\n",
    "            'valid_voxel_idx': valid_en_voxel_idx,\n",
    "            'joint_pred_acc': en_joint_pred_acc\n",
    "        },\n",
    "        'zh': {\n",
    "            'config': zh_config,\n",
    "            'stat': zh_stat,\n",
    "            'valid_voxel_mask': valid_zh_voxel_mask,\n",
    "            'valid_voxel_idx': valid_zh_voxel_idx,\n",
    "            'joint_pred_acc': zh_joint_pred_acc\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_timescale_data(subject_id):\n",
    "    subject_file_en = f\".temp/config/bling/subject/{subject_id}_en.json\"\n",
    "    subject_file_zh = f\".temp/config/bling/subject/{subject_id}_zh.json\"\n",
    "\n",
    "    trainer_en_ts_file = (\n",
    "        f\".temp/config/bling/train/stepwise/{subject_id.lower()}_en_timescale.json\"\n",
    "    )\n",
    "\n",
    "    trainer_zh_ts_file = (\n",
    "        f\".temp/config/bling/train/stepwise/{subject_id.lower()}_zh_timescale.json\"\n",
    "    )\n",
    "\n",
    "    feature_file_en_ts = f\".temp/config/bling/feature/{subject_id}/mBERT_all_untrimmed_timescale_stepwise_en.json\"\n",
    "    feature_file_zh_ts = f\".temp/config/bling/feature/{subject_id}/mBERT_all_untrimmed_timescale_stepwise_zh.json\"\n",
    "\n",
    "    # loading timescale vem\n",
    "    en_meta_df = read_result_meta(\n",
    "        result_meta_dir,\n",
    "        trainer_setting_path=trainer_en_ts_file,\n",
    "        subject_setting_path=subject_file_en,\n",
    "        feature_setting_path=feature_file_en_ts,)\n",
    "\n",
    "    zh_meta_df = read_result_meta(\n",
    "        result_meta_dir,\n",
    "        trainer_setting_path=trainer_zh_ts_file,\n",
    "        subject_setting_path=subject_file_zh,\n",
    "        feature_setting_path=feature_file_zh_ts,\n",
    "    )\n",
    "\n",
    "    # load trainer json file to TrainerSetting\n",
    "    en_ts_config = en_meta_df.iloc[0].to_dict()\n",
    "    zh_ts_config = zh_meta_df.iloc[0].to_dict()\n",
    "        \n",
    "    en_ts_stat = en_ts_config[\"stats_path\"]\n",
    "    en_ts_stat = np.load(en_ts_stat)\n",
    "\n",
    "    zh_ts_stat = zh_ts_config[\"stats_path\"]\n",
    "    zh_ts_stat = np.load(zh_ts_stat)\n",
    "\n",
    "    valid_en_ts_voxel_mask, valid_en_ts_voxel_idx = get_valid_voxels(en_ts_stat, metric=result_metric, score_threshold=r2_threshold, alpha=alpha)\n",
    "    valid_zh_ts_voxel_mask, valid_zh_ts_voxel_idx = get_valid_voxels(zh_ts_stat, metric=result_metric, score_threshold=r2_threshold, alpha=alpha)\n",
    "\n",
    "    en_timescale = en_ts_stat[f\"test_{result_metric}_selectivity_mask\"]\n",
    "    zh_timescale = zh_ts_stat[f\"test_{result_metric}_selectivity_mask\"]\n",
    "\n",
    "    return {\n",
    "        'en': en_timescale,\n",
    "        'zh': zh_timescale,\n",
    "\n",
    "        'en_valid_idx': valid_en_ts_voxel_idx,\n",
    "        'zh_valid_idx': valid_zh_ts_voxel_idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primal_weight(semantic_weight_data):\n",
    "    en_primal_weight = semantic_weight_data[\"en\"][\"config\"][\"primal_weights_path\"]\n",
    "    zh_primal_weight = semantic_weight_data[\"zh\"][\"config\"][\"primal_weights_path\"]\n",
    "\n",
    "    en_primal_weight = np.load(en_primal_weight)[\"primal_weights\"]\n",
    "    zh_primal_weight = np.load(zh_primal_weight)[\"primal_weights\"]\n",
    "\n",
    "    en_primal_weight = np.squeeze(en_primal_weight)\n",
    "    zh_primal_weight = np.squeeze(zh_primal_weight)\n",
    "\n",
    "    en_primal_weight = process_primal_weight(\n",
    "        en_primal_weight,\n",
    "        prediction_score=semantic_weight_data[\"en\"][\"joint_pred_acc\"],\n",
    "        normalize=True,\n",
    "    )\n",
    "    zh_primal_weight = process_primal_weight(\n",
    "        zh_primal_weight,\n",
    "        prediction_score=semantic_weight_data[\"zh\"][\"joint_pred_acc\"],\n",
    "        normalize=True,\n",
    "    )\n",
    "\n",
    "    return {\"en\": en_primal_weight, \"zh\": zh_primal_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales = {}\n",
    "for subject_id in subject_ids:\n",
    "    timescales[subject_id] = load_timescale_data(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_maps = {}\n",
    "for subject_id in subject_ids:\n",
    "    semantic_maps[subject_id] = load_semantic_weight_data(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "primal_weights = {}\n",
    "for subject_id in subject_ids:\n",
    "    primal_weights[subject_id] = get_primal_weight(semantic_maps[subject_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading fasttext embeddings\n",
    "centroids = np.load(cluster_centroids_file)\n",
    "centroids = centroids[[0,1,3,4], :]\n",
    "\n",
    "n_clusters = centroids.shape[0]\n",
    "\n",
    "en_fasttext = \".temp/misc/word_list/en_fasttext_embeddings.npy\"\n",
    "zh_fasttext = \".temp/misc/word_list/zh_fasttext_embeddings.npy\"\n",
    "\n",
    "en_fasttext = np.load(en_fasttext, allow_pickle=True).tolist()\n",
    "zh_fasttext = np.load(zh_fasttext, allow_pickle=True).tolist()\n",
    "\n",
    "en_words = list(en_fasttext.keys())\n",
    "zh_words = list(zh_fasttext.keys())\n",
    "\n",
    "# get word from en_words and zh_words\n",
    "en_words = np.array(en_words)\n",
    "zh_words = np.array(zh_words)\n",
    "\n",
    "# now get all value from en_fasttext dictionary and project that to pca\n",
    "en_fasttext_values = np.array(list(en_fasttext.values()))\n",
    "zh_fasttext_values = np.array(list(zh_fasttext.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_weights(primal_weight, centroids):\n",
    "    def cluster(x, centroids):\n",
    "        return np.argmin(np.linalg.norm(x - centroids, axis=1))\n",
    "    \n",
    "    en_cluster = np.array([cluster(x, centroids) for x in primal_weight['en']])\n",
    "    zh_cluster = np.array([cluster(x, centroids) for x in primal_weight['zh']])\n",
    "\n",
    "    return {\n",
    "        'en': en_cluster,\n",
    "        'zh': zh_cluster\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_weights = {}\n",
    "for subject_id in subject_ids:\n",
    "    clustered_weights[subject_id] = cluster_weights(primal_weights[subject_id], centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timescale_per_cluster(clustered_weight, timescale, semantic_map, n_clusters = 5, join_mode='all'):\n",
    "    en_cluster_voxels = clustered_weight['en']\n",
    "    zh_cluster_voxels = clustered_weight['zh']\n",
    "\n",
    "    en_timescale = timescale['en']\n",
    "    zh_timescale = timescale['zh']\n",
    "\n",
    "    en_semantic_valid_idx = semantic_map['en']['valid_voxel_idx']\n",
    "    zh_semantic_valid_idx = semantic_map['zh']['valid_voxel_idx']\n",
    "\n",
    "    en_timescale_valid_idx = timescale['en_valid_idx']\n",
    "    zh_timescale_valid_idx = timescale['zh_valid_idx']\n",
    "    \n",
    "    if join_mode == 'all':\n",
    "        joint_valid_idx = np.intersect1d(en_semantic_valid_idx, zh_semantic_valid_idx)\n",
    "        joint_valid_idx = np.intersect1d(joint_valid_idx, en_timescale_valid_idx)\n",
    "        joint_valid_idx = np.intersect1d(joint_valid_idx, zh_timescale_valid_idx)\n",
    "\n",
    "        en_valid_idx = joint_valid_idx\n",
    "        zh_valid_idx = joint_valid_idx\n",
    "    else:\n",
    "        en_valid_idx = np.intersect1d(en_semantic_valid_idx, en_timescale_valid_idx)\n",
    "        zh_valid_idx = np.intersect1d(zh_semantic_valid_idx, zh_timescale_valid_idx)\n",
    "    \n",
    "    timescale_per_cluster = {}\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        en_cluster_idx = np.where(en_cluster_voxels == i)[0]\n",
    "        zh_cluster_idx = np.where(zh_cluster_voxels == i)[0]\n",
    "\n",
    "        joint_en_idx = np.intersect1d(en_cluster_idx, en_valid_idx)\n",
    "        joint_zh_idx = np.intersect1d(zh_cluster_idx, zh_valid_idx)\n",
    "\n",
    "        en_cluster_timescale = en_timescale[joint_en_idx]\n",
    "        zh_cluster_timescale = zh_timescale[joint_zh_idx]\n",
    "\n",
    "        timescale_per_cluster[i] = {\n",
    "            'en': en_cluster_timescale,\n",
    "            'zh': zh_cluster_timescale\n",
    "        }\n",
    "    \n",
    "    return timescale_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timescales_per_clusters = {}\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    timescales_per_clusters[subject_id] = get_timescale_per_cluster(\n",
    "        clustered_weights[subject_id], timescales[subject_id], semantic_maps[subject_id], n_clusters=n_clusters, join_mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(timescales_per_clusters, subject_id, axs = None, cmap='hls', plot_xlabel=False, plot_ylabel=False, n_clusters=5):\n",
    "    cmap = create_discrete_cmap(n_clusters, palette=cmap)\n",
    "\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(n_clusters, 1, figsize=(15, 15), sharex=True,)\n",
    "    # share x axis\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        en_timescale = timescales_per_clusters[subject_id][i]['en']\n",
    "        zh_timescale = timescales_per_clusters[subject_id][i]['zh']\n",
    "\n",
    "        # plot en zh but different line style\n",
    "        sns.kdeplot(en_timescale, ax=axs[i], label='en', color=cmap(i), linestyle='--')\n",
    "        sns.kdeplot(zh_timescale, ax=axs[i], label='zh', color=cmap(i), linestyle='-')   \n",
    "        \n",
    "        \n",
    "        axs[i].set_xlim(8, 256)\n",
    "    \n",
    "    # remove xticks for all but last\n",
    "    for i in range(n_clusters-1):\n",
    "        axs[i].set_xticklabels([])\n",
    "    for i in range(n_clusters):\n",
    "        axs[i].set_ylabel('')\n",
    "    \n",
    "    # remove y label for all but the middle\n",
    "    if plot_ylabel:\n",
    "        for i in range(n_clusters):\n",
    "            if i != 2:\n",
    "                axs[i].set_ylabel('')\n",
    "            else:\n",
    "                axs[i].set_ylabel('Density')\n",
    "    \n",
    "    # set subject id as title for first plot\n",
    "    axs[0].set_title(subject_id)\n",
    "\n",
    "    if plot_xlabel:\n",
    "        axs[n_clusters-1].set_xlabel('Timescale Selectivity (words)')\n",
    "    \n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2749, 300)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fasttext_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(\n",
    "    fasttext_pca: np.ndarray, cluster_centers: np.ndarray\n",
    "):\n",
    "    similarity = cosine_similarity(fasttext_pca, cluster_centers)\n",
    "    return similarity\n",
    "\n",
    "def compute_euclidean_distance(\n",
    "    fasttext_pca: np.ndarray, cluster_centers: np.ndarray\n",
    "):\n",
    "    similarity = euclidean_distances(fasttext_pca, cluster_centers)\n",
    "    return similarity\n",
    "\n",
    "def plot_text_pca(ax=None):\n",
    "    en_cos_sim = compute_cosine_similarity(en_fasttext_values, centroids)\n",
    "    zh_cos_sim = compute_cosine_similarity(zh_fasttext_values, centroids)\n",
    "    # now for each center, pick 10 words that has highest cosine similarity\n",
    "    n_words = 20\n",
    "\n",
    "    en_words_idx = np.argsort(en_cos_sim, axis=0)[-n_words:][::-1]\n",
    "    zh_words_idx = np.argsort(zh_cos_sim, axis=0)[-n_words:][::-1]\n",
    "\n",
    "    # get the cluster idx for each word\n",
    "    en_cluster_idx = np.argmax(en_cos_sim, axis=1)\n",
    "    zh_cluster_idx = np.argmax(zh_cos_sim, axis=1)\n",
    "\n",
    "    # get cluster from en_words_idx \n",
    "    en_cluster_words = en_words[en_words_idx]\n",
    "    zh_cluster_words = zh_words[zh_words_idx]\n",
    "\n",
    "    # get vector for each cluster\n",
    "    en_word_vector = en_fasttext_values[en_words_idx]\n",
    "    zh_word_vector = zh_fasttext_values[zh_words_idx]\n",
    "\n",
    "\n",
    "    # do pca and plot\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    en_word_vector_flattened = en_word_vector.reshape(-1, en_word_vector.shape[-1])\n",
    "    zh_word_vector_flattened = zh_word_vector.reshape(-1, zh_word_vector.shape[-1])\n",
    "\n",
    "    # join them \n",
    "    word_vector = np.vstack([en_fasttext_values, zh_fasttext_values])\n",
    "\n",
    "    pca.fit(word_vector)\n",
    "\n",
    "    en_word_vector_pca = pca.transform(en_word_vector_flattened)\n",
    "    zh_word_vector_pca = pca.transform(zh_word_vector_flattened)\n",
    "\n",
    "    # reshape them back\n",
    "    en_word_vector_pca = en_word_vector_pca.reshape(en_word_vector.shape[0], en_word_vector.shape[1], 2)\n",
    "    zh_word_vector_pca = zh_word_vector_pca.reshape(zh_word_vector.shape[0], zh_word_vector.shape[1], 2)\n",
    "\n",
    "    skip = 3\n",
    "    en_word_vector_pca = en_word_vector_pca[::skip, :]\n",
    "    zh_word_vector_pca = zh_word_vector_pca[::skip, :]\n",
    "\n",
    "    en_cluster_words = en_cluster_words[::skip, :]\n",
    "    zh_cluster_words = zh_cluster_words[::skip, :]\n",
    "\n",
    "    from matplotlib import font_manager\n",
    "\n",
    "    font_path = \".temp/font/SourceHanSerifK-Light.otf\"\n",
    "    font_manager.fontManager.addfont(font_path)\n",
    "    prop = font_manager.FontProperties(fname=font_path)\n",
    "\n",
    "\n",
    "    # now plot en_cluster_words and zh_cluster_words in pca space\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    cmap = create_discrete_cmap(n_clusters, palette='hls')\n",
    "\n",
    "    texts = []\n",
    "    for i in range(n_clusters):\n",
    "        en_cluster_pca = en_word_vector_pca[:,i,:].squeeze()\n",
    "        zh_cluster_pca = zh_word_vector_pca[:,i,:].squeeze()\n",
    "\n",
    "        ax.scatter(en_cluster_pca[:, 0], en_cluster_pca[:, 1], label=f\"en_cluster_{i}\", color=cmap(i), s=0)\n",
    "        ax.scatter(zh_cluster_pca[:, 0], zh_cluster_pca[:, 1], label=f\"zh_cluster_{i}\", color=cmap(i), s=0)\n",
    "\n",
    "        for j, txt in enumerate(en_cluster_words[:, i].squeeze()):\n",
    "            # skip if txt len ==1\n",
    "            if len(txt) > 1:\n",
    "                texts.append(ax.annotate(txt, (en_cluster_pca[j, 0], en_cluster_pca[j, 1]), color=cmap(i)))\n",
    "        \n",
    "        for j, txt in enumerate(zh_cluster_words[:,i].squeeze()):\n",
    "            texts.append(ax.annotate(txt, (zh_cluster_pca[j, 0], zh_cluster_pca[j, 1]), fontproperties=prop, color=cmap(i)))\n",
    "        \n",
    "        # for j, txt in enumerate(en_cluster_words[:, i].squeeze()):\n",
    "        #     texts.append(ax.text(en_cluster_pca[j, 0], en_cluster_pca[j, 1], txt, color=cmap(i),))\n",
    "        # for j, txt in enumerate(zh_cluster_words[:,i].squeeze()):\n",
    "        #     texts.append(ax.text(zh_cluster_pca[j, 0], zh_cluster_pca[j, 1], txt, color=cmap(i), fontproperties=prop))\n",
    "            \n",
    "    adjust_text(texts, alw=1.0)\n",
    "    ax.set_ylabel('PC 2')\n",
    "    ax.set_xlabel('PC 1')\n",
    "    # remove ticks\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=figsize_dict['double'])\n",
    "\n",
    "# gridspec\n",
    "gs = fig.add_gridspec(12, 4, width_ratios=[0.8, 0.2, 0.2, 0.2])\n",
    "\n",
    "ax_en_flatmap= fig.add_subplot(gs[:4, 0])\n",
    "ax_zh_flatmap = fig.add_subplot(gs[4:8, 0])\n",
    "\n",
    "en_flatmap = plt.imread(os.path.join(image_dir, f\"COL/en_cluster.png\"))\n",
    "zh_flatmap = plt.imread(os.path.join(image_dir, f\"COL/zh_cluster.png\"))\n",
    "\n",
    "ax_en_flatmap.imshow(en_flatmap)\n",
    "ax_zh_flatmap.imshow(zh_flatmap)\n",
    "\n",
    "ax_en_flatmap.axis('off')\n",
    "ax_zh_flatmap.axis('off')\n",
    "\n",
    "# now plot text pca\n",
    "ax_text_pca = fig.add_subplot(gs[8:, 0])\n",
    "\n",
    "plot_text_pca(ax=ax_text_pca)\n",
    "\n",
    "# now second row for COL histogram, \n",
    "col_hist_axs = [fig.add_subplot(gs[i*3:(i+1)*3, 1]) for i in range(n_clusters)]\n",
    "col_hist_axs = plot_histograms(timescales_per_clusters, 'COL', axs=col_hist_axs, plot_ylabel=True, n_clusters=n_clusters)\n",
    "\n",
    "# now second row for GFW histogram,\n",
    "gfw_hist_axs = [fig.add_subplot(gs[i*3:(i+1)*3, 2]) for i in range(n_clusters)]\n",
    "gfw_hist_axs = plot_histograms(timescales_per_clusters, 'GFW', axs=gfw_hist_axs, plot_xlabel=True, n_clusters=n_clusters)\n",
    "\n",
    "# now second row for TYE histogram,\n",
    "tye_hist_axs = [fig.add_subplot(gs[i*3:(i+1)*3, 3]) for i in range(n_clusters)]\n",
    "tye_hist_axs = plot_histograms(timescales_per_clusters, 'TYE', axs=tye_hist_axs, n_clusters=n_clusters)\n",
    "\n",
    "# add (a) in the top left corner\n",
    "fig.text(0.01, 0.95, \"(a)\", fontsize=default_font_size*1.5)\n",
    "fig.text(0.4725, 0.95, \"(b)\", fontsize=default_font_size*1.5)\n",
    "fig.text(0.01, 0.5, \"(c)\", fontsize=default_font_size*1.5)\n",
    "\n",
    "# add EN and ZH label\n",
    "fig.text(0.26, 0.9, \"EN\", fontsize=default_font_size)\n",
    "fig.text(0.26, 0.6, \"ZH\", fontsize=default_font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_fn = os.path.join(image_dir, \"timescale_cluster_agg.png\")\n",
    "\n",
    "plt.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
